{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx@ 1 alexandrelacheze  staff    21254426 May 24 21:34 \u001b[31m./input/cliente_tabla.csv\u001b[m\u001b[m\r\n",
      "-rwxrwxrwx@ 1 alexandrelacheze  staff      109549 May 24 21:35 \u001b[31m./input/producto_tabla.csv\u001b[m\u001b[m\r\n",
      "-rwxrwxrwx@ 1 alexandrelacheze  staff   251114289 May 24 21:30 \u001b[31m./input/test.csv\u001b[m\u001b[m\r\n",
      "-rw-r--r--@ 1 alexandrelacheze  staff       29179 Jun  7 23:16 ./input/town_state.csv\r\n",
      "-rwxrwxrwx@ 1 alexandrelacheze  staff  3199358223 May 24 21:29 \u001b[31m./input/train.csv\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ./input/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "MAX_NROWS = 100000\n",
    "\n",
    "def read_sample_csv(filename, sample_size=MAX_NROWS, **kwargs):\n",
    "    nrows = sum(1 for line in open(filename)) -1\n",
    "    print \"we got %s rows in %s\" % (nrows, filename)\n",
    "    if nrows < sample_size:\n",
    "        print \"reading everything\"\n",
    "        return pd.read_csv(filename, **kwargs)\n",
    "    else:\n",
    "        print \"sampling %s rows\" % sample_size\n",
    "        skip = sorted(random.sample(xrange(1, nrows+1),nrows-sample_size))\n",
    "        return pd.read_csv(filename, skiprows=skip, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we got 74180464 rows in ./input/train.csv\n",
      "sampling 100000 rows\n",
      "we got 6999251 rows in ./input/test.csv\n",
      "sampling 100000 rows\n"
     ]
    }
   ],
   "source": [
    "# borrowed from https://www.kaggle.com/ericcouto/grupo-bimbo-inventory-demand/using-82-less-memory/code\n",
    "types = {\n",
    "    'Semana':np.uint8,\n",
    "    'Agencia_ID':np.uint16,\n",
    "    'Canal_ID':np.uint8,\n",
    "    'Ruta_SAK':np.uint16,\n",
    "    'Cliente_ID':np.uint32,\n",
    "    'Producto_ID':np.uint32,\n",
    "    'Demanda_uni_equil':np.uint32\n",
    "}\n",
    "\n",
    "train = read_sample_csv('./input/train.csv', usecols=types.keys(), dtype=types)\n",
    "\n",
    "types['id'] = np.uint32\n",
    "del types['Demanda_uni_equil']\n",
    "test = read_sample_csv('./input/test.csv', usecols=types.keys(), dtype=types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Semana</th>\n",
       "      <th>Agencia_ID</th>\n",
       "      <th>Canal_ID</th>\n",
       "      <th>Ruta_SAK</th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>Producto_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218</td>\n",
       "      <td>11</td>\n",
       "      <td>1347</td>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>811505</td>\n",
       "      <td>2425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231</td>\n",
       "      <td>11</td>\n",
       "      <td>1216</td>\n",
       "      <td>1</td>\n",
       "      <td>1625</td>\n",
       "      <td>601148</td>\n",
       "      <td>35141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>10</td>\n",
       "      <td>1347</td>\n",
       "      <td>1</td>\n",
       "      <td>1152</td>\n",
       "      <td>4300673</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>327</td>\n",
       "      <td>11</td>\n",
       "      <td>23669</td>\n",
       "      <td>1</td>\n",
       "      <td>1112</td>\n",
       "      <td>4606367</td>\n",
       "      <td>1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>394</td>\n",
       "      <td>10</td>\n",
       "      <td>3217</td>\n",
       "      <td>1</td>\n",
       "      <td>4406</td>\n",
       "      <td>2178308</td>\n",
       "      <td>32861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Semana  Agencia_ID  Canal_ID  Ruta_SAK  Cliente_ID  Producto_ID\n",
       "0  218      11        1347         1      1263      811505         2425\n",
       "1  231      11        1216         1      1625      601148        35141\n",
       "2  288      10        1347         1      1152     4300673         1309\n",
       "3  327      11       23669         1      1112     4606367         1216\n",
       "4  394      10        3217         1      4406     2178308        32861"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['test'] = 0\n",
    "test['test'] = 1\n",
    "data = pd.concat([train, test])\n",
    "del train\n",
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns= {\n",
    "        \"Agencia_ID\": \"agency_id\",\n",
    "        \"Canal_ID\": \"canal_id\",\n",
    "        \"Cliente_ID\": \"client_id\",\n",
    "        \"Producto_ID\": \"product_id\",\n",
    "        \"Ruta_SAK\": \"route_id\",\n",
    "        \"Semana\": \"week\",\n",
    "        \"Demanda_uni_equil\": \"demand\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_lagging_demand_features(data, max_week=6, inplace=False):\n",
    "    if inplace == False:\n",
    "        data = data.copy()\n",
    "\n",
    "    lag_cols = []\n",
    "    demand_per_CPW = data.groupby([\"client_id\", \"product_id\", \"week\"])[\"demand\"].sum()\n",
    "\n",
    "    for i in range(1, max_week+1):\n",
    "\n",
    "        def get_lagging_demand(row):\n",
    "            client_id, product_id = row[\"client_id\"], row[\"product_id\"]\n",
    "            week = row[\"week\"] - i\n",
    "            \n",
    "            if week <3:\n",
    "                # we don't have anything before week 3\n",
    "                return pd.np.NaN\n",
    "            \n",
    "            if (client_id, product_id, week) in demand_per_CPW.index:\n",
    "                return demand_per_CPW.loc[client_id, product_id, week]\n",
    "            else:\n",
    "                # we assume we got 0 demand for this product, client\n",
    "                return 0\n",
    "    \n",
    "        col_name = \"demand_lag%s\" % i\n",
    "        data[col_name] = data.apply(get_lagging_demand, axis=1)\n",
    "        lag_cols.append(col_name)\n",
    "    \n",
    "    data[\"demand_lagTotal\"] = 0\n",
    "    for col in lag_cols:\n",
    "        data[\"demand_lagTotal\"] += data[col]\n",
    "    lag_cols.append(\"demand_lagTotal\")\n",
    "        \n",
    "    return data, lag_cols\n",
    "\n",
    "def add_frequency_features(data, inplace=False):\n",
    "    if inplace == False:\n",
    "        data = data.copy()\n",
    "        \n",
    "    freq_cols = []\n",
    "    \n",
    "    for aggr_level in [\"agency\", \"route\", \"client\", \"product\"]:\n",
    "        aggr_col = aggr_level+\"_id\"\n",
    "        new_col_name = aggr_level+\"_demand\"\n",
    "        \n",
    "        demand_per_aggr = data.groupby(aggr_col)[\"demand\"].mean().rename(new_col_name).reset_index()\n",
    "        data = pd.merge(data, demand_per_aggr, on=aggr_level+\"_id\",  how='left')\n",
    "        data[new_col_name].fillna(0, inplace=True)\n",
    "        freq_cols.append(new_col_name)\n",
    "    \n",
    "    return data, freq_cols\n",
    "\n",
    "\n",
    "def add_frequency_lagging_features(data, inplace=True, max_week=6):\n",
    "    if inplace == False:\n",
    "        data = data.copy()\n",
    "\n",
    "    freq_lag_cols = []\n",
    "        \n",
    "    for aggr_level in [\"agency\", \"route\", \"client\", \"product\"]:\n",
    "        aggr_col = aggr_level+\"_id\"\n",
    "        new_col_name = aggr_level+\"_demand\"\n",
    "        \n",
    "        demand_per_aggr_and_week = data.groupby([aggr_col, 'week'])[\"demand\"].mean().rename(new_col_name).fillna(0)\n",
    "        lag_cols = []\n",
    "        for i in range(1, max_week+1):\n",
    "\n",
    "            def get_lagging_demand(row):\n",
    "                aggr_index = row[aggr_col]\n",
    "                week = row[\"week\"] - i\n",
    "            \n",
    "                if week <3:\n",
    "                    # we don't have anything before week 3\n",
    "                    return pd.np.NaN\n",
    "            \n",
    "                if (aggr_index, week) in demand_per_aggr_and_week.index:\n",
    "                    return demand_per_aggr_and_week.loc[aggr_index, week]\n",
    "                else:\n",
    "                    # we assume we got 0 demand for this product, client\n",
    "                    return 0\n",
    "    \n",
    "            col_name = \"%s_lag%s\" % (new_col_name, i)\n",
    "            data[col_name] = data.apply(get_lagging_demand, axis=1)\n",
    "\n",
    "            lag_cols.append(col_name)\n",
    "            freq_lag_cols.append(col_name)\n",
    "            \n",
    "        lagTotal_col = \"%s_lagTotal\" % new_col_name\n",
    "        data[lagTotal_col] = 0\n",
    "        for col in lag_cols:\n",
    "            data[lagTotal_col] += data[col]\n",
    "        freq_lag_cols.append(lagTotal_col)\n",
    "        \n",
    "        \n",
    "    return data, freq_lag_cols\n",
    "    \n",
    "def extract_features(data, inplace=False, max_lagging_shift=6):\n",
    "    features_columns = ['agency_id','canal_id','client_id','product_id','route_id']\n",
    "    \n",
    "    # add lagging columns features\n",
    "    data, lag_cols = add_lagging_demand_features(data, inplace=inplace, max_week=max_lagging_shift)\n",
    "    features_columns += lag_cols\n",
    "    \n",
    "    # add frequency features\n",
    "    data, freq_cols = add_frequency_features(data, inplace=inplace)\n",
    "    features_columns += freq_cols\n",
    "    \n",
    "    # add frequency lagging features\n",
    "    data, freq_lag_cols = add_frequency_lagging_features(data, inplace=inplace, max_week=max_lagging_shift)\n",
    "    features_columns += freq_lag_cols\n",
    "    \n",
    "    return data, features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, target_week=10, weeks_used=[7,8,9]):\n",
    "    train = data[data.week.isin(weeks_used)]\n",
    "    test = data[data.week == target_week]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pData, features_columns = extract_features(data, max_lagging_shift=3)\n",
    "pData[\"log_demand\"] = np.log1p(pData[\"demand\"])\n",
    "train, test = train_test_split(pData, target_week=9, weeks_used=[6,7,8])\n",
    "\n",
    "X_train = train[features_columns]\n",
    "X_test = test[features_columns]\n",
    "y_train = train[\"log_demand\"]\n",
    "y_test = test[\"log_demand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41838 entries, 2 to 199998\n",
      "Data columns (total 29 columns):\n",
      "agency_id                  41838 non-null uint16\n",
      "canal_id                   41838 non-null uint8\n",
      "client_id                  41838 non-null uint32\n",
      "product_id                 41838 non-null uint32\n",
      "route_id                   41838 non-null uint16\n",
      "demand_lag1                41838 non-null float64\n",
      "demand_lag2                41838 non-null float64\n",
      "demand_lag3                41838 non-null float64\n",
      "demand_lagTotal            41838 non-null float64\n",
      "agency_demand              41838 non-null float64\n",
      "route_demand               41838 non-null float64\n",
      "client_demand              41838 non-null float64\n",
      "product_demand             41838 non-null float64\n",
      "agency_demand_lag1         41838 non-null float64\n",
      "agency_demand_lag2         41838 non-null float64\n",
      "agency_demand_lag3         41838 non-null float64\n",
      "agency_demand_lagTotal     41838 non-null float64\n",
      "route_demand_lag1          41838 non-null float64\n",
      "route_demand_lag2          41838 non-null float64\n",
      "route_demand_lag3          41838 non-null float64\n",
      "route_demand_lagTotal      41838 non-null float64\n",
      "client_demand_lag1         41838 non-null float64\n",
      "client_demand_lag2         41838 non-null float64\n",
      "client_demand_lag3         41838 non-null float64\n",
      "client_demand_lagTotal     41838 non-null float64\n",
      "product_demand_lag1        41838 non-null float64\n",
      "product_demand_lag2        41838 non-null float64\n",
      "product_demand_lag3        41838 non-null float64\n",
      "product_demand_lagTotal    41838 non-null float64\n",
      "dtypes: float64(24), uint16(2), uint32(2), uint8(1)\n",
      "memory usage: 8.5 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "from sklearn import linear_model, ensemble, svm, naive_bayes\n",
    "from sklearn import metrics\n",
    "\n",
    "#clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "#clf = svm.LinearSVR()\n",
    "clf = ensemble.RandomForestRegressor(n_jobs=-1)\n",
    "#clf = ensemble.GradientBoostingRegressor()\n",
    "#clf = naive_bayes.GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21882849609\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until validation_0 error hasn't decreased in 500 rounds.\n",
      "[0]\tvalidation_0-rmse:1.246617\n",
      "[1]\tvalidation_0-rmse:1.126006\n",
      "[2]\tvalidation_0-rmse:1.040392\n",
      "[3]\tvalidation_0-rmse:0.941423\n",
      "[4]\tvalidation_0-rmse:0.852620\n",
      "[5]\tvalidation_0-rmse:0.773191\n",
      "[6]\tvalidation_0-rmse:0.702645\n",
      "[7]\tvalidation_0-rmse:0.655367\n",
      "[8]\tvalidation_0-rmse:0.597748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.546391734938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9]\tvalidation_0-rmse:0.546391\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xlf = xgb.XGBRegressor(max_depth=10, \n",
    "                        learning_rate=0.1, \n",
    "                        n_estimators=10, \n",
    "                        silent=True, \n",
    "                        objective='reg:linear', \n",
    "                        nthread=-1, \n",
    "                        gamma=0,\n",
    "                        min_child_weight=1, \n",
    "                        max_delta_step=0, \n",
    "                        subsample=0.85, \n",
    "                        colsample_bytree=0.7, \n",
    "                        colsample_bylevel=1, \n",
    "                        reg_alpha=0, \n",
    "                        reg_lambda=1, \n",
    "                        scale_pos_weight=1, \n",
    "                        seed=1440, \n",
    "                        missing=None)\n",
    "\n",
    "xlf.fit(X_train, y_train, eval_metric='rmse', verbose = True, eval_set = [(X_test, y_test)], early_stopping_rounds=500)\n",
    "\n",
    "# calculate the auc score\n",
    "y_pred = xlf.predict(X_test)\n",
    "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels ['demand_pred'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-192f3f1fe482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'demand_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'demand'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'demand_pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'demand_pred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3049\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3051\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3052\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['demand_pred'] not contained in axis"
     ]
    }
   ],
   "source": [
    "# let's predict week 10\n",
    "prep_data10, features_columns10 = extract_features(data, max_lagging_shift=3)\n",
    "prep_data10[\"log_demand\"] = np.log1p(prep_data10[\"demand\"])\n",
    "train10, test10 = train_test_split(prep_data10, target_week=10, weeks_used=[6,7,8,9])\n",
    "\n",
    "X_train10 = train10[features_columns10]\n",
    "X_test10 = test10[features_columns10]\n",
    "y_train10 = train10[\"log_demand\"]\n",
    "\n",
    "clf10 = ensemble.RandomForestRegressor(n_jobs=-1)\n",
    "clf10.fit(X_train10, y_train10)\n",
    "y_pred10 = clf10.predict(X_test10)\n",
    "test10['demand_pred'] = np.exp(y_pred10)-1\n",
    "\n",
    "# add the prediction for week 10 to the data (data2 = data with prdiction of week 10)\n",
    "data2 = pd.merge(data, test10[['id', 'demand_pred']], on='id', how='left')\n",
    "data2['demand'] = data2.apply(lambda row: row['demand_pred'] if row['week'] == 10 else row['demand'],axis=1)\n",
    "data2.drop('demand_pred', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# let's now go over again and predict week 11 with the data with have now (ie with week 10)\\\n",
    "prep_data11, features_columns11 = extract_features(data2, max_lagging_shift=4)\n",
    "prep_data11[\"log_demand\"] = np.log1p(prep_data11[\"demand\"])\n",
    "train11, test11 = train_test_split(prep_data11, target_week=11, weeks_used=[7,8,9, 10])\n",
    "\n",
    "X_train11 = train11[features_columns11]\n",
    "X_test11 = test11[features_columns11]\n",
    "y_train11 = train11[\"log_demand\"]\n",
    "\n",
    "clf11 = ensemble.RandomForestRegressor(n_jobs=-1)\n",
    "clf11.fit(X_train11, y_train11)\n",
    "y_pred11 = clf11.predict(X_test11)\n",
    "test11['demand_pred'] = np.exp(y_pred11)-1\n",
    "\n",
    "# add the prediction for week 11 to the data (data3 = data with prdiction of week and 11)\n",
    "data3 = pd.merge(data2, test11[['id', 'demand_pred']], on='id', how='left')\n",
    "data3['demand'] = data3.apply(lambda row: row['demand_pred'] if row['week'] == 11 else row['demand'], axis=1)\n",
    "data3.drop('demand_pred', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = data3[data3.test == 1][['id', 'demand']]\n",
    "submission.to_csv('./output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>218.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>288.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>327.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>394.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>473.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>481.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>491.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>590.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100009</th>\n",
       "      <td>619.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100010</th>\n",
       "      <td>707.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100011</th>\n",
       "      <td>718.0</td>\n",
       "      <td>8.644125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100012</th>\n",
       "      <td>724.0</td>\n",
       "      <td>5.859373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>855.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100014</th>\n",
       "      <td>934.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100015</th>\n",
       "      <td>1183.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100016</th>\n",
       "      <td>1319.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100017</th>\n",
       "      <td>1399.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100018</th>\n",
       "      <td>1422.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100019</th>\n",
       "      <td>1439.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100020</th>\n",
       "      <td>1698.0</td>\n",
       "      <td>1.880794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100021</th>\n",
       "      <td>1719.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100022</th>\n",
       "      <td>1776.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100023</th>\n",
       "      <td>1787.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100024</th>\n",
       "      <td>1836.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100025</th>\n",
       "      <td>1853.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100026</th>\n",
       "      <td>1910.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100027</th>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100028</th>\n",
       "      <td>1975.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100029</th>\n",
       "      <td>2069.0</td>\n",
       "      <td>17.263166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199970</th>\n",
       "      <td>6997188.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>6997234.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199972</th>\n",
       "      <td>6997401.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199973</th>\n",
       "      <td>6997427.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>6997584.0</td>\n",
       "      <td>13.505246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199975</th>\n",
       "      <td>6997589.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>6997601.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199977</th>\n",
       "      <td>6997789.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199978</th>\n",
       "      <td>6997792.0</td>\n",
       "      <td>5.345342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199979</th>\n",
       "      <td>6997823.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199980</th>\n",
       "      <td>6997881.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199981</th>\n",
       "      <td>6997915.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199982</th>\n",
       "      <td>6997963.0</td>\n",
       "      <td>1.232246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>6998036.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>6998086.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>6998172.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>6998224.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>6998240.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>6998309.0</td>\n",
       "      <td>4.091988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>6998464.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>6998474.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>6998561.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>6998678.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>6998732.0</td>\n",
       "      <td>4.724522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>6998825.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>6998826.0</td>\n",
       "      <td>2.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>6998893.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>6998896.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6999102.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>6999143.0</td>\n",
       "      <td>5.248278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id     demand\n",
       "100000      218.0   0.000000\n",
       "100001      231.0   0.000000\n",
       "100002      288.0   1.000000\n",
       "100003      327.0   0.000000\n",
       "100004      394.0   0.000000\n",
       "100005      473.0   0.000000\n",
       "100006      481.0   0.000000\n",
       "100007      491.0   0.000000\n",
       "100008      590.0   0.000000\n",
       "100009      619.0   0.000000\n",
       "100010      707.0   0.000000\n",
       "100011      718.0   8.644125\n",
       "100012      724.0   5.859373\n",
       "100013      855.0   0.000000\n",
       "100014      934.0   0.000000\n",
       "100015     1183.0   0.000000\n",
       "100016     1319.0   0.000000\n",
       "100017     1399.0   0.000000\n",
       "100018     1422.0   0.000000\n",
       "100019     1439.0   0.000000\n",
       "100020     1698.0   1.880794\n",
       "100021     1719.0   0.000000\n",
       "100022     1776.0   0.000000\n",
       "100023     1787.0   0.000000\n",
       "100024     1836.0   0.000000\n",
       "100025     1853.0   0.000000\n",
       "100026     1910.0   0.000000\n",
       "100027     1949.0   0.000000\n",
       "100028     1975.0   0.000000\n",
       "100029     2069.0  17.263166\n",
       "...           ...        ...\n",
       "199970  6997188.0   0.000000\n",
       "199971  6997234.0   0.000000\n",
       "199972  6997401.0   0.000000\n",
       "199973  6997427.0   0.000000\n",
       "199974  6997584.0  13.505246\n",
       "199975  6997589.0   0.000000\n",
       "199976  6997601.0   0.000000\n",
       "199977  6997789.0   0.000000\n",
       "199978  6997792.0   5.345342\n",
       "199979  6997823.0   0.000000\n",
       "199980  6997881.0   0.000000\n",
       "199981  6997915.0   2.000000\n",
       "199982  6997963.0   1.232246\n",
       "199983  6998036.0   0.000000\n",
       "199984  6998086.0   0.000000\n",
       "199985  6998172.0   0.000000\n",
       "199986  6998224.0   0.000000\n",
       "199987  6998240.0   0.000000\n",
       "199988  6998309.0   4.091988\n",
       "199989  6998464.0   0.000000\n",
       "199990  6998474.0   0.000000\n",
       "199991  6998561.0   0.000000\n",
       "199992  6998678.0   0.000000\n",
       "199993  6998732.0   4.724522\n",
       "199994  6998825.0   0.000000\n",
       "199995  6998826.0   2.999996\n",
       "199996  6998893.0   0.000000\n",
       "199997  6998896.0   0.000000\n",
       "199998  6999102.0   0.000000\n",
       "199999  6999143.0   5.248278\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
